{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and processing data from Hugging Face ðŸ¤—\n",
    "\n",
    "```bash \n",
    "pip install datasets\n",
    "```\n",
    "\n",
    "- Si quieres crear tu propio dataset o compartirlo con otros, puedes seguir la guÃ­a de cÃ³mo aÃ±adir un dataset al Hub de Hugging Face .\n",
    "\n",
    "## Inspecting a dataset ðŸ”Ž\n",
    "\n",
    "DatasetInfo object can contains the description, features and datasetsize. \n",
    "\n",
    "You can access without downloading the dataset.\n",
    "\n",
    "> **Note:** Image and Audio datasets have additional dependencies\n",
    ">```bash\n",
    ">pip install datasets[audio]\n",
    ">pip install datasets[vision]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset_builder\n",
    "DATASET_NAME = 'poloclub/diffusiondb'\n",
    "ds_diffusion = load_dataset_builder(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_diffusion.info.description)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Features](https://huggingface.co/docs/datasets/v2.12.0/about_dataset_features)\n",
    "- Value - int,float...\n",
    "- ClassLabel - Stores as integers\n",
    "- Sequence - Object\n",
    "- Array\n",
    "- Image\n",
    "- Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(ds_diffusion.info.features, indent=4, width=15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split = subset \n",
    "\n",
    "test, train, validation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import get_dataset_split_names\n",
    "get_dataset_split_names(DATASET_NAME)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration = sub-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import get_dataset_config_names\n",
    "get_dataset_config_names(DATASET_NAME)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Datasets ðŸŒŽ vs IterableDatasets ðŸŒŒ](https://huggingface.co/docs/datasets/about_mapstyle_vs_iterable)\n",
    "\n",
    "### ðŸ“– Datasets: use random access and memory-mapping (optimize for memory use) \n",
    "\n",
    "### ðŸ’§ IterableDatasets: use sequential access, don't have to downloading completly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "CONFIGURATION = '2m_first_1k'\n",
    "diffusiondb = load_dataset(DATASET_NAME, CONFIGURATION, split='train')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> **Note:** For large datasets use the index first and the column later\n",
    ">```bash\n",
    ">   dataset[0]['text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "random_i = np.random.choice(range(diffusiondb.num_rows))\n",
    "\n",
    "wrapped_text = textwrap.fill(diffusiondb['prompt'][random_i], width=200)\n",
    "print(wrapped_text)\n",
    "\n",
    "image = diffusiondb['image'][random_i]\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "def show_images(diff_images):\n",
    "    fig, axes = pyplot.subplots(1, 3, figsize=(12,4))\n",
    "    for image, ax in zip(diff_images, axes.ravel()):\n",
    "        ax.imshow(image)\n",
    "    fig.subplots_adjust(wspace=0.2)\n",
    "\n",
    "#show_images(diffusiondb['image'][random_i: 3 + random_i])\n",
    "show_images(diffusiondb[random_i: 3 + random_i]['image'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Pre-processing](https://huggingface.co/docs/datasets/process) ðŸ’½"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_subset = diffusiondb.filter(lambda sample: ' cat ' in sample['prompt'])\n",
    "len(filter_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(filter_subset[:3]['image'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shards\n",
    "\n",
    "- To fit the datasets to the memory resources\n",
    "- Distributed processing of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusiondb.shard(num_shards=2, index=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export\n",
    "\n",
    "Allowed formats:\n",
    "- csv\n",
    "- json\n",
    "- parquet\n",
    "- sql\n",
    "- pandas\n",
    "- dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusiondb.to_parquet('dataset/export/diffusiondb.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Arrow ðŸª¶\n",
    "\n",
    "- Arrow allows zero-copy reads which removes virtually all serialization overhead.\n",
    "- Arrow is language-agnostic (C, C++, C#, Go, Java, JavaScript, Julia, MATLAB, Python, R, Ruby, and Rust).\n",
    "- Arrow is column-oriented so it is faster at querying and processing slices or columns of data.\n",
    "- Arrow can be passed directly to ML tools such as NumPy, Pandas, PyTorch, and TensorFlow.\n",
    "- Arrow supports many, possibly nested, column types.\n",
    "\n",
    "But....\n",
    "\n",
    "\n",
    "Model needs numbers !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusiondb.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusiondb = diffusiondb.remove_columns(['user_name', 'timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusiondb.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplers = diffusiondb.unique('sampler')\n",
    "print(samplers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\"k_euler_ancestral\": 0, \"k_lms\": 1, \"k_euler\": 2, \"k_heun\": 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusiondb_aligned = diffusiondb.align_labels_with_mapping(label2id, \"sampler\")\n",
    "diffusiondb_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusiondb[:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel\n",
    "new_sampler_feat = diffusiondb.features.copy()\n",
    "new_sampler_feat['sampler'] = ClassLabel(names=['k_euler_ancestral', 'k_lms', 'k_euler', 'k_heun'])\n",
    "diffusiondb = diffusiondb.cast(new_sampler_feat)\n",
    "diffusiondb.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusiondb[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusiondb = diffusiondb.align_labels_with_mapping(label2id, \"sampler\")\n",
    "diffusiondb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash \n",
    "pip install tokenizers\n",
    "```\n",
    "\n",
    "[AutoTokenizer](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer)\n",
    "\n",
    "The tokenizer returns a dictionary with three items:\n",
    "\n",
    "- input_ids: the numbers representing the tokens in the text.\n",
    "- token_type_ids: indicates which sequence a token belongs to if there is more than one sequence.\n",
    "- attention_mask: indicates whether a token should be masked or not. The value is 1 for tokens that should be attended to and 0 for padding tokens that should be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"halffried/sd2-laion-clipH14-tokenizer\")\n",
    "\n",
    "print(tokenizer(diffusiondb[\"prompt\"][random_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diffusiondb[\"prompt\"][random_i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map\n",
    "\n",
    "Apply a function to each example in a dataset \n",
    "- independently\n",
    "- batches: Input size != output size BUT all values in the output dictionary must contain the same number of elements\n",
    "\n",
    "\n",
    "Multiprocessing - with_rank "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(sample):\n",
    "    return tokenizer(sample['prompt'])\n",
    "\n",
    "diffusiondb = diffusiondb.map(tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusiondb.features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Image Augmentations](https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Grayscale, CenterCrop\n",
    "from albumentations import HorizontalFlip\n",
    "\n",
    "gray = Grayscale()\n",
    "\n",
    "def transforms(samples):\n",
    "    samples['gray_image'] = [gray(img) for img in samples['image'] ]#for _ in range(2)]\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusiondb = diffusiondb.map(transforms, batched=True) #, remove_columns=[\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusiondb['gray_image'][random_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusiondb.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusiondb.reset_format()\n",
    "\n",
    "diffusiondb.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"gray_image\"])\n",
    "\n",
    "diffusiondb.format['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusiondb.save_to_disk(\"/home/djm/Documents/Hugging Face Workshops/Datasets/dataset/save2disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "diffusiondb.reset_format()\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
    "tf_dataset = diffusiondb.to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"gray_image\"],\n",
    "    label_cols=[\"labels\"],\n",
    "    batch_size=2,\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set_transform - on the fly\n",
    "\n",
    "- user-defined formatting, replaces datasets.Dataset.set_format() \n",
    "- A function that takes a batch (as a dict) as input and returns a batch. \n",
    "- Applied right before returning the objects in getitem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "\n",
    "\n",
    "augmentation_pipeline = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Rotate(limit=30, p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "])\n",
    "\n",
    "def pipeline_transforms(samples):\n",
    "    augmented_image = []\n",
    "    for img in samples['image']:\n",
    "        np_image = np.flip(np.array(img), -1) #np.array(img.convert(\"RGB\"))[:, :, ::-1]\n",
    "\n",
    "        transformed_image = augmentation_pipeline(image=np_image)['image']\n",
    "\n",
    "        tensor_image = torch.tensor(transformed_image).flip(-1).permute(2, 0, 1)\n",
    "        \n",
    "        augmented_image.append(to_pil_image(tensor_image))\n",
    "    \n",
    "    samples['augmented_image'] = augmented_image\n",
    "\n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusiondb.set_transform(pipeline_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "show_images(diffusiondb[random_i:random_i+3]['augmented_image'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MedellÃ­n AI - Meetup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
